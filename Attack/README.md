# Attack     
LBFGS-[201312_Intriguing properties of neural networks](https://arxiv.org/pdf/1312.6199.pdf)                           
FGSM-[201412_EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES](https://arxiv.org/pdf/1412.6572.pdf)                
JSMA-[201511_The Limitations of Deep Learning in Adversarial Settings](https://arxiv.org/pdf/1511.07528.pdf)                
HC-[2016_Adversarial Diversity and Hard Positive Generation](https://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w12/papers/Rozsa_Adversarial_Diversity_and_CVPR_2016_paper.pdf)    
ILL-[201607_Adversarial examples in the physical world](https://arxiv.org/pdf/1607.02533.pdf)                    
Autoencoder-[2017_Alleviating adversarial attacks via convolutional autoencoder](https://ieeexplore.ieee.org/abstract/document/8022700)   
PGD-[201706_Towards deep learning models resistant to adversarial attacks](https://arxiv.org/pdf/1706.06083.pdf)         
EOT-[201707_Synthesizing Robust Adversarial Examples](https://arxiv.org/pdf/1707.07397.pdf)                    
AF-[201812_Adversarial Framing for Image and Video Classification](https://arxiv.org/pdf/1812.04599.pdf)                 
FineFool-[201812_FineFool Fine Object Contour Attack via Attention](https://arxiv.org/pdf/1812.01713.pdf)                        
